{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "widespread-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "royal-feedback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17570497 0.81667436 0.81373083 0.45365642]\n",
      " [0.65507156 0.9182954  0.44150857 0.9319078 ]\n",
      " [0.46075487 0.61216577 0.08352444 0.43859082]\n",
      " [0.20147648 0.34532974 0.41958065 0.19458197]\n",
      " [0.80030846 0.79636807 0.5785543  0.60433818]\n",
      " [0.39232041 0.16670067 0.46795182 0.64587489]\n",
      " [0.37149397 0.38602793 0.57724609 0.77117301]\n",
      " [0.44276678 0.61618945 0.08221358 0.92478675]\n",
      " [0.72859033 0.41533477 0.68552604 0.87852771]\n",
      " [0.53891444 0.30762412 0.18869989 0.34281981]]\n",
      "tf.Tensor([1 3 1 2 0 3 3 3 3 0], shape=(10,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]], shape=(10, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "goal_size = 4\n",
    "batch_size = 10\n",
    "example_goal = np.random.rand(batch_size, goal_size)\n",
    "np.random.shuffle(example_goal)\n",
    "print(example_goal)\n",
    "arg_max = tf.argmax(example_goal, axis=1, output_type=tf.int32)\n",
    "print(arg_max)\n",
    "one_hot = tf.one_hot(arg_max, depth=1)\n",
    "print(one_hot)\n",
    "# # example_goal[0][0] = 1\n",
    "\n",
    "# np.random.shuffle(example_goal)\n",
    "# print(example_goal, '\\n', tf.argmax(example_goal))\n",
    "\n",
    "# # np.random.shuffle(example_goal)\n",
    "# example_goal = tf.convert_to_tensor(example_goal)\n",
    "# print(tf.argmax(example_goal, 1), tf.reduce_max(example_goal))\n",
    "# highest_vals_per_col = tf.argmax(example_goal, 1)\n",
    "# print(highest_vals_per_col, highest_vals_per_col.shape)\n",
    "\n",
    "# # max_value = max(example_goal)\n",
    "# # max_index = my_list.index(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "spare-sally",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Reduction axis 0 is empty in shape [0] [Op:ArgMax]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-81a9f50731fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     place_h[i] = [example_goal[0][a]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mone_hot_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplace_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_goal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moff_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlvenv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlvenv/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36margmax_v2\u001b[0;34m(input, axis, output_type, name)\u001b[0m\n\u001b[1;32m    303\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlvenv/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36marg_max\u001b[0;34m(input, dimension, output_type, name)\u001b[0m\n\u001b[1;32m    819\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m       return arg_max_eager_fallback(\n\u001b[0m\u001b[1;32m    822\u001b[0m           input, dimension, output_type=output_type, name=name, ctx=_ctx)\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlvenv/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36marg_max_eager_fallback\u001b[0;34m(input, dimension, output_type, name, ctx)\u001b[0m\n\u001b[1;32m    852\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tidx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m   _result = _execute.execute(b\"ArgMax\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0m\u001b[1;32m    855\u001b[0m                              ctx=ctx, name=name)\n\u001b[1;32m    856\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rlvenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Reduction axis 0 is empty in shape [0] [Op:ArgMax]"
     ]
    }
   ],
   "source": [
    "place_h = []\n",
    "# for i in range(goal_size):\n",
    "#     print(i, example_goal.shape)\n",
    "#     a = highest_vals_per_col.data[i]\n",
    "#     print[a]\n",
    "#     place_h[i] = [example_goal[0][a]]\n",
    "    \n",
    "one_hot_idx = tf.argmax(place_h)\n",
    "print(one_hot_idx)\n",
    "one_hot = tf.one_hot(tf.argmax(example_goal), depth=4, dtype=float, on_value=1.0, off_value=0.0)\n",
    "print(one_hot)\n",
    "one_hot = tf.squeeze(one_hot)\n",
    "print(one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "faced-zimbabwe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289\n",
      "[[1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]] \n",
      " 3\n",
      "17 0 17 0\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]] \n",
      " (24, 24)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-5038fc32548b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_up\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_down\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpad_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnfz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnfz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnfz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "x,y = 17,17\n",
    "a = np.ones((7,7))\n",
    "# a[1][1] = 1\n",
    "map_ = np.zeros((18,18))\n",
    "\n",
    "\n",
    "\n",
    "shape_total = map_.shape\n",
    "shape_loc = a.shape\n",
    "np.random.shuffle(a)\n",
    "nfz = int((shape_loc[0]-1)/2)\n",
    "print(a, \"\\n\", nfz)\n",
    "\n",
    "pad_left = x\n",
    "pad_right = shape_total[0] - x -1# - shape_loc[0] + nfz\n",
    "pad_up = y # - nfz\n",
    "pad_down = shape_total[0] - y - 1# - shape_loc[0] + nfz\n",
    "\n",
    "print(pad_left, pad_right, pad_up, pad_down)\n",
    "\n",
    "padded = np.pad(a, ((pad_up, pad_down), (pad_left, pad_right)))\n",
    "print(padded, \"\\n\", padded.shape)\n",
    "padded = padded[nfz:(padded.shape[0]-nfz), nfz:(padded[1]-nfz)]\n",
    "print(padded, padded.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "republican-thumb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] \n",
      " (19, 19)\n"
     ]
    }
   ],
   "source": [
    "# def pad_centered(state, map_in, pad_value):\n",
    "map_in = np.zeros((10,10))\n",
    "print(map_in.shape)\n",
    "padding_rows = math.ceil(map_in.shape[0] / 2.0)\n",
    "padding_cols = math.ceil(map_in.shape[1] / 2.0)\n",
    "position_x, position_y = 0,0\n",
    "map_in[position_x][position_y] = 1\n",
    "pad_value = 1\n",
    "# print(\"pos\", position_x, position_y)\n",
    "position_row_offset = padding_rows - position_y\n",
    "position_col_offset = padding_cols - position_x\n",
    "res = np.pad(map_in,\n",
    "              pad_width=[[padding_rows + position_row_offset - 1, padding_rows - position_row_offset],\n",
    "                         [padding_cols + position_col_offset - 1, padding_cols - position_col_offset],\n",
    "#                          [0, 0]\n",
    "                        ],\n",
    "                  mode='constant',\n",
    "                  constant_values=pad_value)\n",
    "\n",
    "print(res,\"\\n\", res.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "south-arena",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]] \n",
      " True\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]] \n",
      " [[1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((6,6))\n",
    "b = np.zeros((6,6))\n",
    "\n",
    "for i in range(5):\n",
    "    a[i][i]=1\n",
    "np.random.shuffle(a)\n",
    "b[0][0]=1\n",
    "print(a, \"\\n\", bool(b.any))\n",
    "# a = not bool(a)\n",
    "\n",
    "a = np.ones((6,6))\n",
    "a = np.logical_not(a).astype(int)\n",
    "b = b.astype(int)\n",
    "print(a, \"\\n\", b)\n",
    "\n",
    "c = b*a\n",
    "print(c)\n",
    "# c = np.logical_not(c).astype(int)\n",
    "print(c)\n",
    "print(not np.all(c == 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "subtle-jewelry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289\n",
      "81\n",
      "208\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "lm_size = (17,17)\n",
    "NT_size = (9,9)\n",
    "\n",
    "print(17**2)\n",
    "print(9**2)\n",
    "print(17**2-9**2)\n",
    "a = np.zeros(lm_size)\n",
    "print(a)\n",
    "\n",
    "\n",
    "\n",
    "# NT_size[0]:(lm_size[0]-NT_size[0]), NT_size[0]:(lm_size[0]-NT_size[0])\n",
    "\n",
    "a[9-5:17-4,9-5:17-4] = 1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "varying-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [10.3, 22.3, 1.1, 2.34, 0]\n",
    "l2 = [1.3, 2.3, 10.1, 20.34, 330]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "received-printing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False  True]] (32, 32)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def pad_lm_to_total_size(h_target, position):\n",
    "    \"\"\"\n",
    "    pads input of shape local_map to output of total_map_size\n",
    "    \"\"\"\n",
    "\n",
    "    shape_map = (32,32)\n",
    "    shape_htarget = h_target.shape\n",
    "    # print(shape_htarget, shape_map)\n",
    "\n",
    "    x, y = position\n",
    "\n",
    "    pad_left = x\n",
    "    pad_right = shape_map[0] - x - 1\n",
    "    pad_up = y\n",
    "    pad_down = shape_map[1] - y - 1\n",
    "\n",
    "    padded = np.pad(h_target, ((pad_up, pad_down), (pad_left, pad_right)))\n",
    "\n",
    "    lm_as_tm_size = padded[int((shape_htarget[0] - 1) / 2):int(padded.shape[0] - (shape_htarget[0] - 1) / 2),\n",
    "                    int((shape_htarget[1] - 1) / 2):int(padded.shape[1] - (shape_htarget[1] - 1) / 2)]\n",
    "\n",
    "    return lm_as_tm_size.astype(bool)\n",
    "\n",
    "position = (31,31)\n",
    "h_target = np.zeros((15,15))\n",
    "h_target[7][7] = 1\n",
    "\n",
    "pht = pad_lm_to_total_size(h_target, position)\n",
    "print(pht, pht.shape)\n",
    "\n",
    "print(pht.any()==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "physical-clark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((10,5))\n",
    "\n",
    "a[9,3]=1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "excess-wagon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((10,10))\n",
    "a[0,1]=1\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input \n",
    "input_img = Input(shape=(128, 128, 3))#Encoder \n",
    "y = Conv2D(32, (3, 3), padding='same',strides =(2,2))(input_img)\n",
    "y = LeakyReLU()(y)\n",
    "y = Conv2D(64, (3, 3), padding='same',strides =(2,2))(y)\n",
    "y = LeakyReLU()(y)\n",
    "y1 = Conv2D(128, (3, 3), padding='same',strides =(2,2))(y) # skip-1\n",
    "y = LeakyReLU()(y1)\n",
    "y = Conv2D(256, (3, 3), padding='same',strides =(2,2))(y)\n",
    "y = LeakyReLU()(y)\n",
    "y2 = Conv2D(256, (3, 3), padding='same',strides =(2,2))(y)# skip-2\n",
    "y = LeakyReLU()(y2)\n",
    "y = Conv2D(512, (3, 3), padding='same',strides =(2,2))(y)\n",
    "y = LeakyReLU()(y)\n",
    "y = Conv2D(1024, (3, 3), padding='same',strides =(2,2))(y)\n",
    "y = LeakyReLU()(y)#Flattening for the bottleneck\n",
    "vol = y.shape\n",
    "x = Flatten()(y)\n",
    "latent = Dense(128, activation='relu')(x) \n",
    "\n",
    "\n",
    "# Helper function to apply activation and batch normalization to the # output added with output of residual connection from the encoderdef lrelu_bn(inputs):\n",
    "   lrelu = LeakyReLU()(inputs)\n",
    "   bn = BatchNormalization()(lrelu)\n",
    "   return bn#Decoder\n",
    "y = Dense(np.prod(vol[1:]), activation='relu')(latent)\n",
    "y = Reshape((vol[1], vol[2], vol[3]))(y)\n",
    "y = Conv2DTranspose(1024, (3,3), padding='same')(y)\n",
    "y = LeakyReLU()(y)\n",
    "y = Conv2DTranspose(512, (3,3), padding='same',strides=(2,2))(y)\n",
    "y = LeakyReLU()(y)\n",
    "y = Conv2DTranspose(256, (3,3), padding='same',strides=(2,2))(y)\n",
    "y= Add()([y2, y]) # second skip connection added here\n",
    "y = lrelu_bn(y)\n",
    "y = Conv2DTranspose(256, (3,3), padding='same',strides=(2,2))(y)\n",
    "y = LeakyReLU()(y)\n",
    "y = Conv2DTranspose(128, (3,3), padding='same',strides=(2,2))(y)\n",
    "y= Add()([y1, y]) # first skip connection added here\n",
    "y = lrelu_bn(y)\n",
    "y = Conv2DTranspose(64, (3,3), padding='same',strides=(2,2))(y)\n",
    "y = LeakyReLU()(y)\n",
    "y = Conv2DTranspose(32, (3,3), padding='same',strides=(2,2))(y)\n",
    "y = LeakyReLU()(y)\n",
    "y = Conv2DTranspose(3, (3,3), activation='sigmoid', padding='same',strides=(2,2))(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6c8e6734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1296) (None, 1296)\n",
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_157 (InputLayer)          [(None, 1, 17, 17, 4 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_158 (InputLayer)          [(None, 1, 21, 21, 4 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_local_conv_1 (Conv2D)  (None, 1, 15, 15, 4) 148         input_157[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_global_conv_1 (Conv2D) (None, 1, 17, 17, 4) 404         input_158[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_local_conv_2 (Conv2D)  (None, 1, 13, 13, 8) 296         hl_model_local_conv_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_global_map_2 (Conv2D)  (None, 1, 13, 13, 8) 808         hl_model_global_conv_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_local_conv_3 (Conv2D)  (None, 1, 11, 11, 16 1168        hl_model_local_conv_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_global_map_3 (Conv2D)  (None, 1, 9, 9, 16)  3216        hl_model_global_map_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_local_conv_4 (Conv2D)  (None, 1, 9, 9, 16)  2320        hl_model_local_conv_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_global_flatten (Flatte (None, 1296)         0           hl_model_global_map_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_local_flatten (Flatten (None, 1296)         0           hl_model_local_conv_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_concat_flatten (Concat (None, 2592)         0           hl_model_global_flatten[0][0]    \n",
      "                                                                 hl_model_local_flatten[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "input_159 (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_concat (Concatenate)   (None, 2593)         0           hl_model_concat_flatten[0][0]    \n",
      "                                                                 input_159[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_last_dense_layer_hl (D (None, 300)          778200      hl_model_concat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_last_dense_layer (Resh (None, 5, 5, 12)     0           hl_model_last_dense_layer_hl[0][0\n",
      "__________________________________________________________________________________________________\n",
      "hl_model_deconv_1 (Conv2DTransp (None, 9, 9, 16)     4816        hl_model_last_dense_layer[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze_32 (TFOpLa (None, 9, 9, 16)     0           hl_model_local_conv_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_1st_skip_connection_co (None, 9, 9, 32)     0           hl_model_deconv_1[0][0]          \n",
      "                                                                 tf.compat.v1.squeeze_32[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_deconv_2 (Conv2DTransp (None, 11, 11, 8)    2312        hl_model_1st_skip_connection_conc\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze_33 (TFOpLa (None, 11, 11, 16)   0           hl_model_local_conv_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_2nd_skip_connection_co (None, 11, 11, 24)   0           hl_model_deconv_2[0][0]          \n",
      "                                                                 tf.compat.v1.squeeze_33[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_deconv_2.1 (Conv2DTran (None, 13, 13, 8)    1736        hl_model_2nd_skip_connection_conc\n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze_34 (TFOpLa (None, 13, 13, 8)    0           hl_model_local_conv_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_hidden_layer_all_hl_0  (None, 256)          664064      hl_model_concat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_3rd_skip_connection_co (None, 13, 13, 16)   0           hl_model_deconv_2.1[0][0]        \n",
      "                                                                 tf.compat.v1.squeeze_34[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_hidden_layer_all_hl_1  (None, 512)          131584      hl_model_hidden_layer_all_hl_0[0]\n",
      "__________________________________________________________________________________________________\n",
      "hl_model_deconv_3 (Conv2DTransp (None, 17, 17, 4)    1604        hl_model_3rd_skip_connection_conc\n",
      "__________________________________________________________________________________________________\n",
      "hl_model_hidden_layer_all_hl_2  (None, 256)          131328      hl_model_hidden_layer_all_hl_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "hl_model_deconv_4 (Conv2DTransp (None, 17, 17, 1)    5           hl_model_deconv_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_landing_preproc_layer_ (None, 128)          32896       hl_model_hidden_layer_all_hl_2[0]\n",
      "__________________________________________________________________________________________________\n",
      "hl_model_deconv_flatten (Flatte (None, 289)          0           hl_model_deconv_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "hl_model_landing_layer_hl (Dens (None, 1)            129         hl_model_landing_preproc_layer_hl\n",
      "__________________________________________________________________________________________________\n",
      "hl_model_concat_final (Concaten (None, 290)          0           hl_model_deconv_flatten[0][0]    \n",
      "                                                                 hl_model_landing_layer_hl[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,757,034\n",
      "Trainable params: 1,757,034\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def lrelu(inputs):\n",
    "    lrelu = LeakyReLU()(inputs)\n",
    "    bn = BatchNormalization()(lrelu)\n",
    "    return bn\n",
    "\n",
    "conv_layers = 2\n",
    "mb = 25\n",
    "current_mb = 15\n",
    "hidden_layer_size = 256\n",
    "name = 'hl_model_'\n",
    "lm = np.random.rand(17,17,4)\n",
    "gm = np.random.rand(21,21,4)\n",
    "states_proc = np.array(current_mb/mb)\n",
    "\n",
    "\n",
    "def build_hl_model(local_map, global_map, states_proc): #local:17,17,4; global:21:21,4\n",
    "    \n",
    "    # local map processing layers\n",
    "#     for k in range(conv_layers):\n",
    "    local_map_input = tf.keras.layers.Input(shape=local_map.shape)\n",
    "    global_map_input = tf.keras.layers.Input(shape=global_map.shape)\n",
    "    states_proc_input = tf.keras.layers.Input(shape=states_proc.shape)\n",
    "    \n",
    "    local_map_1 = tf.keras.layers.Conv2D(4, 3, activation='elu',\n",
    "                       strides=(1, 1),\n",
    "                       name=name + 'local_conv_' + str(0 + 1))(local_map_input) #out:(None, 1, 15, 15, 4) 1156->\n",
    "    local_map_2 = tf.keras.layers.Conv2D(8, 3, activation='elu',\n",
    "                       strides=(1, 1),\n",
    "                       name=name + 'local_conv_' + str(1 + 1))(local_map_1) #out:(None, 1, 13, 13, 8)\n",
    "    local_map_3 = tf.keras.layers.Conv2D(16, 3, activation='elu',\n",
    "                       strides=(1, 1),\n",
    "                       name=name + 'local_conv_' + str(2 + 1))(local_map_2) #out:(None, 1, 11, 11, 16)\n",
    "    local_map_4 = tf.keras.layers.Conv2D(16, 3, activation='elu',\n",
    "                       strides=(1, 1),\n",
    "                       name=name + 'local_conv_' + str(3 + 1))(local_map_3) #out:(None, 1, 9, 9, 16)\n",
    "    flatten_local = tf.keras.layers.Flatten(name=name + 'local_flatten')(local_map_4)\n",
    "    \n",
    "    # global map processing layers\n",
    "\n",
    "    global_map_1 = tf.keras.layers.Conv2D(4, 5, activation='elu',\n",
    "                       strides=(1, 1),\n",
    "                       name=name + 'global_conv_' + str(0 + 1))(global_map_input) #out:17\n",
    "    global_map_2 = tf.keras.layers.Conv2D(8, 5, activation='elu',\n",
    "                       strides=(1, 1),\n",
    "                       name=name + 'global_map_' + str(1 + 1))(global_map_1) #out:13\n",
    "    global_map_3 = tf.keras.layers.Conv2D(16, 5, activation='elu',\n",
    "                       strides=(1, 1),\n",
    "                       name=name + 'global_map_' + str(2 + 1))(global_map_2)#out:9\n",
    "\n",
    "    flatten_global = tf.keras.layers.Flatten(name=name + 'global_flatten')(global_map_3)\n",
    "    \n",
    "    print(flatten_local.shape, flatten_global.shape)\n",
    "    \n",
    "    flatten_map = tf.keras.layers.Concatenate(name=name + 'concat_flatten')([flatten_global, flatten_local])\n",
    "    \n",
    "    layer = tf.keras.layers.Concatenate(name=name + 'concat')([flatten_map, states_proc_input])\n",
    "    \n",
    "    layer_1 = tf.keras.layers.Dense(256, activation='elu', name=name + 'hidden_layer_all_hl_' + str(0))(\n",
    "                layer)\n",
    "    layer_2 = tf.keras.layers.Dense(512, activation='elu', name=name + 'hidden_layer_all_hl_' + str(1))(\n",
    "                layer_1)\n",
    "    layer_3 = tf.keras.layers.Dense(256, activation='elu', name=name + 'hidden_layer_all_hl_' + str(2))(\n",
    "                layer_2)\n",
    "\n",
    "    output = tf.keras.layers.Dense(units=300, activation='linear', name=name + 'last_dense_layer_hl')(\n",
    "        layer)\n",
    "    \n",
    "    reshape = tf.keras.layers.Reshape((5,5,12), name=name + 'last_dense_layer')(output)\n",
    "\n",
    "    \n",
    "    landing = tf.keras.layers.Dense(units=128, activation='elu', name=name + 'landing_preproc_layer_hl')(\n",
    "        layer_3)\n",
    "    landing = tf.keras.layers.Dense(units=1, activation='elu', name=name + 'landing_layer_hl')(landing)\n",
    "    \n",
    "    # deconvolutional part aiming at 17x17 \n",
    "    deconv_1 = tf.keras.layers.Conv2DTranspose(filters=16, kernel_size=5, activation='elu', name=name + 'deconv_' + str(1))(reshape)\n",
    "    skip_1 = tf.keras.layers.Concatenate(name=name + '1st_skip_connection_concat', axis=3)([deconv_1, tf.squeeze(local_map_4, axis=1)])\n",
    "    deconv_2 = tf.keras.layers.Conv2DTranspose(filters=8, kernel_size=3, activation='elu', name=name + 'deconv_' + str(2))(skip_1)\n",
    "    skip_2 = tf.keras.layers.Concatenate(name=name + '2nd_skip_connection_concat', axis=3)([deconv_2, tf.squeeze(local_map_3, axis=1)])\n",
    "    deconv_2_1 = tf.keras.layers.Conv2DTranspose(filters=8, kernel_size=3, activation='elu', name=name + 'deconv_' + str(2.1))(skip_2)\n",
    "    skip_3 = tf.keras.layers.Concatenate(name=name + '3rd_skip_connection_concat', axis=3)([deconv_2_1, tf.squeeze(local_map_2, axis=1)])\n",
    "    deconv_3 = tf.keras.layers.Conv2DTranspose(filters=4, kernel_size=5, activation='elu', name=name + 'deconv_' + str(3))(skip_3)\n",
    "    deconv_4 = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=1, activation='elu', name=name + 'deconv_' + str(4))(deconv_3)\n",
    "\n",
    "    flatten_deconv = tf.keras.layers.Flatten(name=name + 'deconv_flatten')(deconv_4)\n",
    "    concat_final = tf.keras.layers.Concatenate(name=name + 'concat_final')([flatten_deconv, landing])\n",
    "    \n",
    "    return tf.keras.Model(inputs=[local_map_input, global_map_input, states_proc_input], outputs=concat_final)\n",
    "    \n",
    "\n",
    "model = build_hl_model(lm[tf.newaxis, ...], gm[tf.newaxis, ...], states_proc[tf.newaxis, ...]) #lm, gm, states_proc)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# model.build()\n",
    "model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fbb20d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[[0 1 2 3 4]\n",
      " [5 6 7 8 9]]\n",
      "tf.Tensor(\n",
      "[[0 1 2 3 4]\n",
      " [5 6 7 8 9]], shape=(2, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(10)\n",
    "print(a)\n",
    "a = a.reshape(2,5)\n",
    "print(a)\n",
    "a = tf.keras.layers.Flatten()(tf.convert_to_tensor(a))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8265061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(2, shape=(), dtype=int32) tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a, b, c = tf.stop_gradient([1,2,3])\n",
    "print( a,b,c\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbda1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
